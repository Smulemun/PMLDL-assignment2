{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "from dgl.nn.pytorch import edge_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAGNN_metapath_specific(nn.Module):\n",
    "    def __init__(self,\n",
    "                 out_dim,\n",
    "                 num_heads,\n",
    "                 attn_drop=0.5,\n",
    "                 alpha=0.01,\n",
    "                 use_minibatch=False,\n",
    "                 attn_switch=False):\n",
    "        super(MAGNN_metapath_specific, self).__init__()\n",
    "        self.out_dim = out_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.use_minibatch = use_minibatch\n",
    "        self.attn_switch = attn_switch\n",
    "\n",
    "        # rnn-like metapath instance aggregator\n",
    "        # consider multiple attention heads\n",
    "        self.rnn = nn.Linear(out_dim, num_heads * out_dim)\n",
    "\n",
    "        # node-level attention\n",
    "        # attention considers the center node embedding or not\n",
    "        if self.attn_switch:\n",
    "            self.attn1 = nn.Linear(out_dim, num_heads, bias=False)\n",
    "            self.attn2 = nn.Parameter(torch.empty(size=(1, num_heads, out_dim)))\n",
    "        else:\n",
    "            self.attn = nn.Parameter(torch.empty(size=(1, num_heads, out_dim)))\n",
    "        self.leaky_relu = nn.LeakyReLU(alpha)\n",
    "        self.softmax = edge_softmax\n",
    "        if attn_drop:\n",
    "            self.attn_drop = nn.Dropout(attn_drop)\n",
    "        else:\n",
    "            self.attn_drop = lambda x: x\n",
    "\n",
    "        # weight initialization\n",
    "        if self.attn_switch:\n",
    "            nn.init.xavier_normal_(self.attn1.weight, gain=1.414)\n",
    "            nn.init.xavier_normal_(self.attn2.data, gain=1.414)\n",
    "        else:\n",
    "            nn.init.xavier_normal_(self.attn.data, gain=1.414)\n",
    "\n",
    "    def edge_softmax(self, g):\n",
    "        attention = self.softmax(g, g.edata.pop('a'))\n",
    "        # Dropout attention scores and save them\n",
    "        g.edata['a_drop'] = self.attn_drop(attention)\n",
    "\n",
    "    def message_passing(self, edges):\n",
    "        ft = edges.data['eft'] * edges.data['a_drop']\n",
    "        return {'ft': ft}\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # features: num_all_nodes x out_dim\n",
    "        if self.use_minibatch:\n",
    "            g, features, type_mask, edge_metapath_indices, target_idx = inputs\n",
    "        else:\n",
    "            g, features, type_mask, edge_metapath_indices = inputs\n",
    "\n",
    "        # Embedding layer\n",
    "        # use torch.nn.functional.embedding or torch.embedding here\n",
    "        # do not use torch.nn.embedding\n",
    "        # edata: E x Seq x out_dim\n",
    "        edata = F.embedding(edge_metapath_indices, features)\n",
    "\n",
    "        # apply rnn to metapath-based feature sequence\n",
    "        hidden = self.rnn(torch.mean(edata, dim=1))\n",
    "        hidden = hidden.unsqueeze(dim=0)\n",
    "\n",
    "        eft = hidden.permute(1, 0, 2).view(-1, self.num_heads, self.out_dim)  # E x num_heads x out_dim\n",
    "        if self.attn_switch:\n",
    "            center_node_feat = F.embedding(edge_metapath_indices[:, -1], features)  # E x out_dim\n",
    "            a1 = self.attn1(center_node_feat)  # E x num_heads\n",
    "            a2 = (eft * self.attn2).sum(dim=-1)  # E x num_heads\n",
    "            a = (a1 + a2).unsqueeze(dim=-1)  # E x num_heads x 1\n",
    "        else:\n",
    "            a = (eft * self.attn).sum(dim=-1).unsqueeze(dim=-1)  # E x num_heads x 1\n",
    "        a = self.leaky_relu(a)\n",
    "        g.edata.update({'eft': eft, 'a': a})\n",
    "        # compute softmax normalized attention values\n",
    "        self.edge_softmax(g)\n",
    "        # compute the aggregated node features scaled by the dropped,\n",
    "        # unnormalized attention values.\n",
    "        g.update_all(self.message_passing, fn.sum('ft', 'ft'))\n",
    "        ret = g.ndata['ft']  # E x num_heads x out_dim\n",
    "\n",
    "        if self.use_minibatch:\n",
    "            return ret[target_idx]\n",
    "        else:\n",
    "            return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAGNN_ctr_ntype_specific(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_metapaths,\n",
    "                 out_dim,\n",
    "                 num_heads,\n",
    "                 attn_vec_dim,\n",
    "                 attn_drop=0.5,\n",
    "                 use_minibatch=False):\n",
    "        super(MAGNN_ctr_ntype_specific, self).__init__()\n",
    "        self.out_dim = out_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.use_minibatch = use_minibatch\n",
    "\n",
    "        # metapath-specific layers\n",
    "        self.metapath_layers = nn.ModuleList()\n",
    "        for i in range(num_metapaths):\n",
    "            self.metapath_layers.append(MAGNN_metapath_specific(out_dim,\n",
    "                                                                num_heads,\n",
    "                                                                attn_drop=attn_drop,\n",
    "                                                                use_minibatch=use_minibatch))\n",
    "\n",
    "        # metapath-level attention\n",
    "        # note that the acutal input dimension should consider the number of heads\n",
    "        # as multiple head outputs are concatenated together\n",
    "        self.fc1 = nn.Linear(out_dim * num_heads, attn_vec_dim, bias=True)\n",
    "        self.fc2 = nn.Linear(attn_vec_dim, 1, bias=False)\n",
    "\n",
    "        # weight initialization\n",
    "        nn.init.xavier_normal_(self.fc1.weight, gain=1.414)\n",
    "        nn.init.xavier_normal_(self.fc2.weight, gain=1.414)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.use_minibatch:\n",
    "            g_list, features, type_mask, edge_metapath_indices_list, target_idx_list = inputs\n",
    "\n",
    "            # metapath-specific layers\n",
    "            metapath_outs = [F.elu(metapath_layer((g, features, type_mask, edge_metapath_indices, target_idx)).view(-1, self.num_heads * self.out_dim))\n",
    "                             for g, edge_metapath_indices, target_idx, metapath_layer in zip(g_list, edge_metapath_indices_list, target_idx_list, self.metapath_layers)]\n",
    "        else:\n",
    "            g_list, features, type_mask, edge_metapath_indices_list = inputs\n",
    "\n",
    "            # metapath-specific layers\n",
    "            metapath_outs = [F.elu(metapath_layer((g, features, type_mask, edge_metapath_indices)).view(-1, self.num_heads * self.out_dim))\n",
    "                             for g, edge_metapath_indices, metapath_layer in zip(g_list, edge_metapath_indices_list, self.metapath_layers)]\n",
    "\n",
    "        beta = []\n",
    "        for metapath_out in metapath_outs:\n",
    "            fc1 = torch.tanh(self.fc1(metapath_out))\n",
    "            fc1_mean = torch.mean(fc1, dim=0)\n",
    "            fc2 = self.fc2(fc1_mean)\n",
    "            beta.append(fc2)\n",
    "        beta = torch.cat(beta, dim=0)\n",
    "        beta = F.softmax(beta, dim=0)\n",
    "        beta = torch.unsqueeze(beta, dim=-1)\n",
    "        beta = torch.unsqueeze(beta, dim=-1)\n",
    "        metapath_outs = [torch.unsqueeze(metapath_out, dim=0) for metapath_out in metapath_outs]\n",
    "        metapath_outs = torch.cat(metapath_outs, dim=0)\n",
    "        h = torch.sum(beta * metapath_outs, dim=0)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAGNN_lp_layer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_metapaths_list,\n",
    "                 in_dim,\n",
    "                 out_dim,\n",
    "                 num_heads,\n",
    "                 attn_vec_dim,\n",
    "                 attn_drop=0.5):\n",
    "        super(MAGNN_lp_layer, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # ctr_ntype-specific layers\n",
    "        self.user_layer = MAGNN_ctr_ntype_specific(num_metapaths_list[0],\n",
    "                                                   in_dim,\n",
    "                                                   num_heads,\n",
    "                                                   attn_vec_dim,\n",
    "                                                   attn_drop,\n",
    "                                                   use_minibatch=True)\n",
    "        self.item_layer = MAGNN_ctr_ntype_specific(num_metapaths_list[1],\n",
    "                                                   in_dim,\n",
    "                                                   num_heads,\n",
    "                                                   attn_vec_dim,\n",
    "                                                   attn_drop,\n",
    "                                                   use_minibatch=True)\n",
    "\n",
    "        # note that the acutal input dimension should consider the number of heads\n",
    "        # as multiple head outputs are concatenated together\n",
    "        self.fc_user = nn.Linear(in_dim * num_heads, out_dim, bias=True)\n",
    "        self.fc_item = nn.Linear(in_dim * num_heads, out_dim, bias=True)\n",
    "        nn.init.xavier_normal_(self.fc_user.weight, gain=1.414)\n",
    "        nn.init.xavier_normal_(self.fc_item.weight, gain=1.414)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        g_lists, features, type_mask, edge_metapath_indices_lists, target_idx_lists = inputs\n",
    "\n",
    "        # user and movie embedding\n",
    "        h_user = self.user_layer(\n",
    "            (g_lists[0], features, type_mask, edge_metapath_indices_lists[0], target_idx_lists[0]))\n",
    "        h_item = self.item_layer(\n",
    "            (g_lists[1], features, type_mask, edge_metapath_indices_lists[1], target_idx_lists[1]))\n",
    "\n",
    "        logits_user = self.fc_user(h_user)\n",
    "        logits_item = self.fc_item(h_item)\n",
    "        return [logits_user, logits_item], [h_user, h_item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAGNN_lp(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_metapaths_list, # [3, 3]\n",
    "                 feats_dim_list, # feature dimensions [10, 20]\n",
    "                 hidden_dim, \n",
    "                 out_dim,\n",
    "                 num_heads, # 8\n",
    "                 attn_vec_dim, # 128\n",
    "                 dropout_rate=0.5):\n",
    "        super(MAGNN_lp, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # ntype-specific transformation\n",
    "        self.fc_list = nn.ModuleList([nn.Linear(feats_dim, hidden_dim, bias=True) for feats_dim in feats_dim_list])\n",
    "        # feature dropout after trainsformation\n",
    "        if dropout_rate > 0:\n",
    "            self.feat_drop = nn.Dropout(dropout_rate)\n",
    "        else:\n",
    "            self.feat_drop = lambda x: x\n",
    "        # initialization of fc layers\n",
    "        for fc in self.fc_list:\n",
    "            nn.init.xavier_normal_(fc.weight, gain=1.414)\n",
    "\n",
    "        # MAGNN_lp layers\n",
    "        self.layer1 = MAGNN_lp_layer(num_metapaths_list,\n",
    "                                     hidden_dim,\n",
    "                                     out_dim,\n",
    "                                     num_heads,\n",
    "                                     attn_vec_dim,\n",
    "                                     attn_drop=dropout_rate)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        g_lists, features_list, type_mask, edge_metapath_indices_lists, target_idx_lists = inputs\n",
    "\n",
    "        # feature embeddings \n",
    "        transformed_features = torch.zeros(type_mask.shape[0], self.hidden_dim, device=features_list[0].device)\n",
    "        for i, fc in enumerate(self.fc_list):\n",
    "            node_indices = np.where(type_mask == i)[0]\n",
    "            transformed_features[node_indices] = fc(features_list[i])\n",
    "        transformed_features = self.feat_drop(transformed_features)\n",
    "\n",
    "        # hidden layers\n",
    "        [logits_user, logits_item], [h_user, h_item] = self.layer1(\n",
    "            (g_lists, transformed_features, type_mask, edge_metapath_indices_lists, target_idx_lists))\n",
    "\n",
    "        return [logits_user, logits_item], [h_user, h_item]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
