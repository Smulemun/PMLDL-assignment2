{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_USERS = 943\n",
    "NUM_MOVIES = 1682\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "num_user_features = 24\n",
    "num_movie_features = 19\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/preprocessed/'\n",
    "\n",
    "in_file = open(path + 'user_features.pickle', 'rb')\n",
    "user_features = pickle.load(in_file)\n",
    "for key, val in user_features.items():\n",
    "    user_features[key] = torch.tensor(val.astype(np.float32))\n",
    "in_file.close()\n",
    "\n",
    "in_file = open(path + 'movie_features.pickle', 'rb')\n",
    "movie_features = pickle.load(in_file)\n",
    "for key, val in movie_features.items():\n",
    "    movie_features[key] = torch.tensor(val.astype(np.float32))\n",
    "in_file.close()\n",
    "\n",
    "in_file = open(path + 'u_m_u_metapath_map.pickle', 'rb')\n",
    "user_metapaths = pickle.load(in_file)\n",
    "in_file.close()\n",
    "\n",
    "in_file = open(path + 'm_u_m_metapath_map.pickle', 'rb')\n",
    "movie_metapaths = pickle.load(in_file)\n",
    "in_file.close()\n",
    "\n",
    "train_pos = np.load(path + 'train_pos.npy')\n",
    "test_pos = np.load(path + 'test_pos.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserMovieDataset(Dataset):\n",
    "    def __init__(self, positives, user_metapaths, movie_metapaths):\n",
    "\n",
    "        self.max_metapath_size = 7000\n",
    "        self.user_metapaths = user_metapaths\n",
    "        self.movie_metapaths = movie_metapaths\n",
    "        self.merge_metapaths()\n",
    "\n",
    "        self.pos = positives\n",
    "        self.sample_negatives()\n",
    "\n",
    "        self.data = torch.tensor(np.vstack([self.pos, self.neg]))\n",
    "        self.labels = torch.vstack([torch.ones((len(self.pos), 1)), torch.zeros((len(self.neg), 1))])\n",
    "\n",
    "    def sample_negatives(self):\n",
    "        self.neg = []\n",
    "        for i in tqdm(range(len(self.pos)), desc='Sampling negative edges'):\n",
    "            user_id = np.random.randint(NUM_USERS)\n",
    "            movie_id = np.random.randint(NUM_MOVIES)\n",
    "            while ([user_id, movie_id] not in self.pos and \\\n",
    "                    [user_id, movie_id] not in self.neg) or \\\n",
    "                    len(self.user_metapaths[user_id]) == 0 or \\\n",
    "                    len(self.movie_metapaths[movie_id]) == 0:\n",
    "                user_id = np.random.randint(NUM_USERS)\n",
    "                movie_id = np.random.randint(NUM_MOVIES)\n",
    "            self.neg.append([user_id, movie_id])\n",
    "        self.neg = np.array(self.neg)\n",
    "\n",
    "    def merge_metapaths(self):\n",
    "        for key, val in tqdm(self.user_metapaths.items(), desc='Extracting user metapaths'):\n",
    "            if len(self.user_metapaths[key]) > 0:\n",
    "                self.user_metapaths[key] = [torch.vstack([user_features[i] for i in val[:, 0]])[:self.max_metapath_size], \n",
    "                                            torch.vstack([movie_features[i] for i in val[:, 1]])[:self.max_metapath_size], \n",
    "                                            torch.vstack([user_features[i] for i in val[:, 2]])[:self.max_metapath_size]]\n",
    "                while len(self.user_metapaths[key][0]) < self.max_metapath_size:\n",
    "                    self.user_metapaths[key][0] = torch.vstack([self.user_metapaths[key][0], torch.zeros_like(self.user_metapaths[key][0][0])])\n",
    "                    self.user_metapaths[key][1] = torch.vstack([self.user_metapaths[key][1], torch.zeros_like(self.user_metapaths[key][1][0])])\n",
    "                    self.user_metapaths[key][2] = torch.vstack([self.user_metapaths[key][2], torch.zeros_like(self.user_metapaths[key][2][0])])\n",
    "                \n",
    "            \n",
    "        for key, val in tqdm(self.movie_metapaths.items(), desc='Extracting movie metapaths'):\n",
    "            if len(self.movie_metapaths[key]) > 0:\n",
    "                self.movie_metapaths[key] = [torch.vstack([movie_features[i] for i in val[:, 0]])[:self.max_metapath_size],\n",
    "                                            torch.vstack([user_features[i] for i in val[:, 1]])[:self.max_metapath_size],\n",
    "                                            torch.vstack([movie_features[i] for i in val[:, 2]])[:self.max_metapath_size]]\n",
    "                while len(self.movie_metapaths[key][0]) < self.max_metapath_size:\n",
    "                    self.movie_metapaths[key][0] = torch.vstack([self.movie_metapaths[key][0], torch.zeros_like(self.movie_metapaths[key][0][0])])\n",
    "                    self.movie_metapaths[key][1] = torch.vstack([self.movie_metapaths[key][1], torch.zeros_like(self.movie_metapaths[key][1][0])])\n",
    "                    self.movie_metapaths[key][2] = torch.vstack([self.movie_metapaths[key][2], torch.zeros_like(self.movie_metapaths[key][2][0])])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_id, movie_id = self.data[idx]\n",
    "        return self.data[idx], self.labels[idx], *self.user_metapaths[user_id.item()], *self.movie_metapaths[movie_id.item()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting user metapaths: 100%|██████████| 943/943 [11:24<00:00,  1.38it/s]\n",
      "Extracting movie metapaths: 100%|██████████| 1682/1682 [57:37<00:00,  2.06s/it] \n",
      "Sampling negative edges: 100%|██████████| 49906/49906 [00:26<00:00, 1886.91it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = UserMovieDataset(train_pos, user_metapaths, movie_metapaths)\n",
    "# test_dataset = UserMovieDataset(test_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetapathEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dim,\n",
    "                 out_dim, \n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, metapath):\n",
    "        # print(f'{metapath.shape=}')\n",
    "        x = torch.mean(metapath, dim=1)\n",
    "        # print(f'{x.shape=}')\n",
    "        return self.fc(x)\n",
    "\n",
    "class MAGNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                #  user_features,\n",
    "                #  movie_features,\n",
    "                #  user_metapaths,\n",
    "                #  movie_metapaths,\n",
    "                 num_user_features,\n",
    "                 num_movie_features,\n",
    "                 hidden_dim,\n",
    "                 out_dim,\n",
    "                 batch_size,\n",
    "                 device\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        # later take only train/test features and metapaths\n",
    "\n",
    "        # self.user_features = user_features \n",
    "        # self.movie_features = movie_features\n",
    "\n",
    "        # self.user_metapaths = user_metapaths\n",
    "        # self.movie_metapaths = movie_metapaths\n",
    "\n",
    "        # self.num_user_features = len(user_features[0])\n",
    "        # self.num_movie_features = len(movie_features[0])\n",
    "\n",
    "        self.num_user_features = num_user_features\n",
    "        self.num_movie_features = num_movie_features\n",
    "\n",
    "        self.user_feature_encoder = nn.Linear(self.num_user_features, hidden_dim)\n",
    "        self.movie_feature_encoder = nn.Linear(self.num_movie_features, hidden_dim)\n",
    "\n",
    "        self.metapath_encoder = MetapathEncoder(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.user_node_embedding = nn.Linear(hidden_dim, out_dim)\n",
    "        self.movie_node_embedding = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "        self.recommender = nn.Linear(2 * out_dim, 1)\n",
    "\n",
    "    def forward(self, edge, user_metapaths1, user_metapaths2, user_metapaths3, movie_metapaths1, movie_metapaths2, movie_metapaths3):\n",
    "        # user_node, movie_node = edge[:, 0], edge[:, 1]\n",
    "        # batch_aggregated_user_metapath = []\n",
    "        # for i in range(self.batch_size):\n",
    "        #     user_metapath_instances = user_metapaths[user_node[i].item()]\n",
    "        #     user_aggregated_metapath = torch.zeros(self.hidden_dim).to(self.device)\n",
    "        #     for metapath in user_metapath_instances:\n",
    "        #         metapath_isntance = torch.vstack([ # optimize\n",
    "        #             self.user_feature_encoder(self.user_features[metapath[0]]),\n",
    "        #             self.movie_feature_encoder(self.movie_features[metapath[1]]),\n",
    "        #             self.user_feature_encoder(self.user_features[metapath[2]])\n",
    "        #         ])\n",
    "        #         user_aggregated_metapath += self.metapath_encoder(metapath_isntance)\n",
    "        #     batch_aggregated_user_metapath.append(user_aggregated_metapath)\n",
    "        user_metapath_isntance = torch.cat([\n",
    "            self.user_feature_encoder(user_metapaths1),\n",
    "            self.movie_feature_encoder(user_metapaths2),\n",
    "            self.user_feature_encoder(user_metapaths3)\n",
    "        ], dim=1)\n",
    "        print(f'{user_metapath_isntance.shape=}')\n",
    "        user_aggregated_metapath = self.metapath_encoder(user_metapath_isntance)\n",
    "        print(f'{user_aggregated_metapath.shape=}')\n",
    "        # batch_aggregated_movie_metapath = []\n",
    "        # for i in range(self.batch_size):\n",
    "        #     movie_metapath_instances = movie_metapaths[movie_node[i].item()]\n",
    "        #     movie_aggregated_metapath = torch.zeros(self.hidden_dim).to(self.device)\n",
    "        #     for metapath in movie_metapath_instances:\n",
    "        #         metapath_isntance = torch.vstack([\n",
    "        #             self.movie_feature_encoder(self.movie_features[metapath[0]]),\n",
    "        #             self.user_feature_encoder(self.user_features[metapath[1]]),\n",
    "        #             self.movie_feature_encoder(self.movie_features[metapath[2]]),\n",
    "        #         ])\n",
    "        #         movie_aggregated_metapath += self.metapath_encoder(metapath_isntance)\n",
    "        #     batch_aggregated_movie_metapath.append(movie_aggregated_metapath)\n",
    "        # batch_aggregated_movie_metapath = torch.vstack(batch_aggregated_movie_metapath)\n",
    "\n",
    "        movie_metapath_isntance = torch.cat([\n",
    "            self.movie_feature_encoder(movie_metapaths1),\n",
    "            self.user_feature_encoder(movie_metapaths2),\n",
    "            self.movie_feature_encoder(movie_metapaths3)\n",
    "        ], dim=1)\n",
    "        print(f'{movie_metapath_isntance.shape=}')\n",
    "        movie_aggregated_metapath = self.metapath_encoder(movie_metapath_isntance)\n",
    "        print(f'{movie_aggregated_metapath.shape=}')\n",
    "\n",
    "        user_embed = F.sigmoid(self.user_node_embedding(user_aggregated_metapath))\n",
    "        movie_embed = F.sigmoid(self.movie_node_embedding(movie_aggregated_metapath))\n",
    "\n",
    "        out = self.recommender(torch.cat([user_embed, movie_embed], dim=1))\n",
    "\n",
    "        return F.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MAGNN(num_user_features, num_movie_features, 128, 128, BATCH_SIZE, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    losses = []\n",
    "    accs = []\n",
    "\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
    "    for batch in progress_bar:\n",
    "        edge, label, umetapath1, umetapath2, umetapath3, mmetapath1, mmetapath2, mmetapath3 = batch\n",
    "        edge, label, umetapath1, umetapath2, umetapath3, mmetapath1, mmetapath2, mmetapath3 = edge.to(device), label.to(device), umetapath1.to(device), umetapath2.to(device), umetapath3.to(device), mmetapath1.to(device), mmetapath2.to(device), mmetapath3.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(edge, umetapath1, umetapath2, umetapath3, mmetapath1, mmetapath2, mmetapath3)\n",
    "\n",
    "        loss = criterion(label, pred)\n",
    "        acc = ((pred > 0.5) == label).sum() / BATCH_SIZE\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        accs.append(acc.cpu())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar.set_description(f'Loss: {np.mean(losses):.5f}, Acc: {np.mean(accs):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/780 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 6.00 GiB total capacity; 1.72 GiB already allocated; 2.67 GiB free; 1.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pokem\\Desktop\\study\\3 year\\PMLDL\\assigment2\\PMLDL-assignment2\\notebooks\\2_MAGNN.ipynb Cell 12\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment2/PMLDL-assignment2/notebooks/2_MAGNN.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment2/PMLDL-assignment2/notebooks/2_MAGNN.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment2/PMLDL-assignment2/notebooks/2_MAGNN.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     train(epoch)\n",
      "\u001b[1;32mc:\\Users\\pokem\\Desktop\\study\\3 year\\PMLDL\\assigment2\\PMLDL-assignment2\\notebooks\\2_MAGNN.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment2/PMLDL-assignment2/notebooks/2_MAGNN.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m edge, label, umetapath1, umetapath2, umetapath3, mmetapath1, mmetapath2, mmetapath3 \u001b[39m=\u001b[39m edge\u001b[39m.\u001b[39mto(device), label\u001b[39m.\u001b[39mto(device), umetapath1\u001b[39m.\u001b[39mto(device), umetapath2\u001b[39m.\u001b[39mto(device), umetapath3\u001b[39m.\u001b[39mto(device), mmetapath1\u001b[39m.\u001b[39mto(device), mmetapath2\u001b[39m.\u001b[39mto(device), mmetapath3\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment2/PMLDL-assignment2/notebooks/2_MAGNN.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment2/PMLDL-assignment2/notebooks/2_MAGNN.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m pred \u001b[39m=\u001b[39m model(edge, umetapath1, umetapath2, umetapath3, mmetapath1, mmetapath2, mmetapath3)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment2/PMLDL-assignment2/notebooks/2_MAGNN.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(label, pred)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment2/PMLDL-assignment2/notebooks/2_MAGNN.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m acc \u001b[39m=\u001b[39m ((pred \u001b[39m>\u001b[39m \u001b[39m0.5\u001b[39m) \u001b[39m==\u001b[39m label)\u001b[39m.\u001b[39msum() \u001b[39m/\u001b[39m BATCH_SIZE\n",
      "File \u001b[1;32mc:\\Users\\pokem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\pokem\\Desktop\\study\\3 year\\PMLDL\\assigment2\\PMLDL-assignment2\\notebooks\\2_MAGNN.ipynb Cell 12\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment2/PMLDL-assignment2/notebooks/2_MAGNN.ipynb#X14sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, edge, user_metapaths1, user_metapaths2, user_metapaths3, movie_metapaths1, movie_metapaths2, movie_metapaths3):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment2/PMLDL-assignment2/notebooks/2_MAGNN.ipynb#X14sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39m# user_node, movie_node = edge[:, 0], edge[:, 1]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment2/PMLDL-assignment2/notebooks/2_MAGNN.ipynb#X14sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     \u001b[39m# batch_aggregated_user_metapath = []\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment2/PMLDL-assignment2/notebooks/2_MAGNN.ipynb#X14sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     \u001b[39m#         user_aggregated_metapath += self.metapath_encoder(metapath_isntance)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment2/PMLDL-assignment2/notebooks/2_MAGNN.ipynb#X14sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     \u001b[39m#     batch_aggregated_user_metapath.append(user_aggregated_metapath)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment2/PMLDL-assignment2/notebooks/2_MAGNN.ipynb#X14sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     user_metapath_isntance \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment2/PMLDL-assignment2/notebooks/2_MAGNN.ipynb#X14sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muser_feature_encoder(user_metapaths1),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment2/PMLDL-assignment2/notebooks/2_MAGNN.ipynb#X14sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmovie_feature_encoder(user_metapaths2),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment2/PMLDL-assignment2/notebooks/2_MAGNN.ipynb#X14sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muser_feature_encoder(user_metapaths3)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment2/PMLDL-assignment2/notebooks/2_MAGNN.ipynb#X14sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     ], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment2/PMLDL-assignment2/notebooks/2_MAGNN.ipynb#X14sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00muser_metapath_isntance\u001b[39m.\u001b[39mshape\u001b[39m=}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pokem/Desktop/study/3%20year/PMLDL/assigment2/PMLDL-assignment2/notebooks/2_MAGNN.ipynb#X14sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     user_aggregated_metapath \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetapath_encoder(user_metapath_isntance)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 6.00 GiB total capacity; 1.72 GiB already allocated; 2.67 GiB free; 1.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
